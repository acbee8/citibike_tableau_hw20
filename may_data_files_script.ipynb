{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "from urllib import *\n",
    "import csv\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cwd = os.getcwd()\n",
    "# newdir = cwd +\"\\\\data\"\n",
    "# print(\"The current directory is \" + cwd)\n",
    "# os.mkdir( newdir, 0777);\n",
    "# print(\"Created new directory \" + newdir)\n",
    "# newfile = open('zipfiles.csv', '')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path =r'C:\\DRO\\DCL_rawdata_files' # use your path\n",
    "# allFiles = glob.glob(path + \"/*.csv\")\n",
    "# frame = pd.DataFrame()\n",
    "# list_ = []\n",
    "# for file_ in allFiles:\n",
    "#     df = pd.read_csv(file_,index_col=None, header=0)\n",
    "#     list_.append(df)\n",
    "# frame = pd.concat(list_)\n",
    "\n",
    "\n",
    "# #Provide the url\n",
    "# url = 'https://s3.amazonaws.com/tripdata/index.html'\n",
    "\n",
    "# #Request the site\n",
    "# page = urllib.request.urlopen(url)\n",
    "\n",
    "# #Parse the html and store in variable\n",
    "# parser = soup(page, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://s3.amazonaws.com/tripdata/201405-citibike-tripdata.zip',\n",
       " 'https://s3.amazonaws.com/tripdata/201505-citibike-tripdata.zip',\n",
       " 'https://s3.amazonaws.com/tripdata/201605-citibike-tripdata.zip',\n",
       " 'https://s3.amazonaws.com/tripdata/201705-citibike-tripdata.zip',\n",
       " 'https://s3.amazonaws.com/tripdata/201805-citibike-tripdata.zip']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # List of dates to pull and base href\n",
    "# may_years = ['201405', '201505', '201605', '201705', '201805']\n",
    "# href_pattern = 'https://s3.amazonaws.com/tripdata/{}-citibike-tripdata.zip'\n",
    "\n",
    "# years_list = []\n",
    "# def get_years():\n",
    "#     for year in may_years:\n",
    "#         href = href_pattern.format(year)\n",
    "#     #   print(href)\n",
    "#         years_list.append(href)        \n",
    "# get_years()\n",
    "# years_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'https://s3.amazonaws.com/tripdata/index.html'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-1fb58eb79356>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<html>data</html\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'https://s3.amazonaws.com/tripdata/index.html'"
     ]
    }
   ],
   "source": [
    "# sourcePage = urlopen(request('https://www.google.com/googlebooks/patents-grants-text.html')) soup = BeautifulSoup(sourcePage.read())\n",
    "\n",
    "# soup = BeautifulSoup(\"<html>data</html\")\n",
    "\n",
    "# for may_href in years_list:\n",
    "#     print('working on' + may_href)\n",
    "#     soup.find(may_href)\n",
    "    \n",
    "\n",
    "# # s = soup.find_all(href=href_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('techtrack100.csv','w', newline='') as f_output:\n",
    "#     csv_output = csv.writer(f_output)\n",
    "#     csv_output.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2014-05-citibike-tripdata.csv\n",
    "# 201505-citibike-tripdata.csv\n",
    "# 201605-citibike-tripdata.csv\n",
    "# 201705-citibike-tripdata.csv\n",
    "# 201805-citibike-tripdata.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "may_2014 = os.path.join('Data', '2014-05-citibike-tripdata.csv')\n",
    "may_2015 = os.path.join('Data', '201505-citibike-tripdata.csv')\n",
    "may_2016 = os.path.join('Data', '201605-citibike-tripdata.csv')\n",
    "may_2017 = os.path.join('Data', '201705-citibike-tripdata.csv')\n",
    "may_2018 = os.path.join('Data', '201805-citibike-tripdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2014 = pd.read_csv(may_2014)\n",
    "df_2015 = pd.read_csv(may_2015)\n",
    "df_2016 = pd.read_csv(may_2016)\n",
    "df_2017 = pd.read_csv(may_2017)\n",
    "df_2018 = pd.read_csv(may_2018)\n",
    "# df_2018.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df_2014, df_2015, df_2016, df_2017, df_2018]\n",
    "df = pd.concat(frames)\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(['index'], axis=1)\n",
    "# df = df.reset_index()\n",
    "df = df.rename(columns={'index': 'rideid'})\n",
    "df.to_csv('may_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
